{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27147a2f-5b23-4c92-851b-24c7b43ccf13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Practical 2c: One-Hot Encoding\n",
    "\n",
    "In this notebook we will be adding additional features to our model, as well as discuss how to handle categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eecccb8-1693-4cc5-bfc0-b9d419721fde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\n",
    "airbnbDF = spark.read.parquet(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e6e9350-5158-4b01-b8dc-faddd67ff0c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Let's use the same 80/20 split with the same seed as the previous notebook so we can compare our results apples to apples (unless you changed the cluster config!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131fd6c7-2e84-4810-9194-603261ebfa79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4ecb6f-8255-40cb-9c42-96db2b9210b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Option 1: StringIndexer, OneHotEncoder, and VectorAssembler\n",
    "\n",
    "Here, we are going to One Hot Encode (OHE) our categorical variables. The first approach we are going to use will combine StringIndexer, OneHotEncoder, and VectorAssembler.\n",
    "\n",
    "First we need to use `StringIndexer` to map a string column of labels to an ML column of label indices [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.StringIndexer).\n",
    "\n",
    "Then, we can apply the `OneHotEncoder` to the output of the StringIndexer [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.OneHotEncoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06b81fe-2591-4ff9-a132-be2ed6e64973",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                           outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70940f32-f42d-4c8b-a638-1ab881aeef12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can combine our OHE categorical features with our numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696e4cc9-0242-4653-ab20-d9afb24747c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d646025a-4b12-40fd-bc04-54cdfa979c82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Option 2: RFormula\n",
    "Instead of manually specifying which columns are categorical to the StringIndexer and OneHotEncoder, RFormula can do that automatically for you [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RFormula)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.RFormula).\n",
    "\n",
    "With RFormula, if you have any columns of type String, it treats it as a categorical feature and string indexes & one hot encodes it for us. Otherwise, it leaves as it is. Then it combines all of one-hot encoded features and numeric features into a single vector, called `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1063ff1d-118e-4da3-bbbb-7b034edca9a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b79f3ee-ef79-4bd9-9895-f2b60ff02439",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Now that we have all of our features, let's build a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2a9d4a-0261-4090-9610-699412411cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceebe97f-09dc-48e9-95c8-a6461c879185",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pipeline\n",
    "\n",
    "Let's put all these stages in a Pipeline. A `Pipeline` is a way of organizing all of our transformers and estimators [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.Pipeline).\n",
    "\n",
    "Verify you get the same results with Option 1 (StringIndexer, OneHotEncoderEstimator, and VectorAssembler) and Option 2 (RFormula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749a031d-e8b5-4310-9d81-a02ddc41b903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n|            features|price|        prediction|\n+--------------------+-----+------------------+\n|(98,[0,3,6,22,43,...| 85.0| 55.86406437022288|\n|(98,[0,3,6,22,43,...| 45.0| 22.87558895398388|\n|(98,[0,3,6,22,43,...| 70.0|27.382602888721067|\n|(98,[0,3,6,12,42,...|128.0|-91.50712171182795|\n|(98,[0,3,6,12,43,...|159.0| 94.66621817641771|\n+--------------------+-----+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Option 1: StringIndexer + OHE + VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = [stringIndexer, oheEncoder, vecAssembler, lr]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a219887f-b888-407a-ab0d-af555f5f4ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n|            features|price|        prediction|\n+--------------------+-----+------------------+\n|(98,[0,3,6,7,23,4...| 85.0| 55.40518338251741|\n|(98,[0,3,6,7,23,4...| 45.0|22.558643930734434|\n|(98,[0,3,6,7,23,4...| 70.0|27.035891181432817|\n|(98,[0,3,6,7,13,4...|128.0|-91.29310091873367|\n|(98,[0,3,6,7,13,4...|159.0| 94.66473870534765|\n+--------------------+-----+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Option 2: RFormula\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aeea7e7-96bc-41d9-a9dd-6647034b939b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluate Model: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c71a0e37-425d-4391-bb1b-627af45473fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.69\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "rmse = round(regressionEvaluator.evaluate(predDF), 2)\n",
    "print(f\"RMSE is {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d0a0acc-4b0d-4bdf-9798-cacbc88e9286",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## R2\n",
    "\n",
    "![](https://files.training.databricks.com/images/r2d2.jpg) How is our R2 doing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7e070e-8e2a-4852-96f8-8ad460a5eed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16\n"
     ]
    }
   ],
   "source": [
    "r2 = round(regressionEvaluator.setMetricName(\"r2\").evaluate(predDF), 2)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2624e1f6-f169-437b-a119-8bbc4fc676ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipelinePath = \"/tmp/sf-airbnb/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b42ed2f-4b6e-4d0f-836b-e66d04fcb81a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading models\n",
    "\n",
    "When you load in models, you need to know the type of model you are loading back in (was it a linear regression or logistic regression model?).\n",
    "\n",
    "For this reason, we recommend you always put your transformers/estimators into a Pipeline, so you can always load the generic PipelineModel back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48e2abd-21ea-44cf-b4e3-56a5ee38d7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd7aeb0a-fc15-48ea-bc9a-4bb9831a2515",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Distributed Setting\n",
    "\n",
    "If you are interested in learning how linear regression is implemented in the distributed setting and bottlenecks, check out these lecture slides:\n",
    "* [distributed-linear-regression-1](https://files.training.databricks.com/static/docs/distributed-linear-regression-1.pdf)\n",
    "* [distributed-linear-regression-2](https://files.training.databricks.com/static/docs/distributed-linear-regression-2.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practical_2c",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
