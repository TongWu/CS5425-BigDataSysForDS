{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3f896f-09af-49d1-897a-5e4c79f8011d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Demo_3: Machine Learning Pipeline\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/ml-pipeline.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "643819e7-fdfa-49d1-a834-222efb673879",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "764bb2e8-9000-4f6f-94a5-8fcb99562f3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52be5c0d-6819-4e1c-a2ea-7ffc5db38de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3db00f7-1131-4252-9f4e-37eb9c771278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3f8840-cae2-4d71-9c1d-20618ae9fdaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+----------------------+----------------------------------------------------------------------------+------------------------------------------+----------+\n|id |text            |label|words                 |features                                                                    |probability                               |prediction|\n+---+----------------+-----+----------------------+----------------------------------------------------------------------------+------------------------------------------+----------+\n|0  |a b c d e spark |1.0  |[a, b, c, d, e, spark]|(262144,[74920,89530,107107,148981,167694,173558],[1.0,1.0,1.0,1.0,1.0,1.0])|[0.002628213496942035,0.9973717865030579] |1.0       |\n|1  |b d             |0.0  |[b, d]                |(262144,[89530,148981],[1.0,1.0])                                           |[0.9963902711801113,0.0036097288198887467]|0.0       |\n|2  |spark f g h     |1.0  |[spark, f, g, h]      |(262144,[36803,173558,209078,228158],[1.0,1.0,1.0,1.0])                     |[0.0022081050570269896,0.997791894942973] |1.0       |\n|3  |hadoop mapreduce|0.0  |[hadoop, mapreduce]   |(262144,[132966,198017],[1.0,1.0])                                          |[0.9987232337063715,0.0012767662936284951]|0.0       |\n+---+----------------+-----+----------------------+----------------------------------------------------------------------------+------------------------------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on train documents and print columns of interest.\n",
    "pred_train = model.transform(training)\n",
    "pred_train.drop('rawPrediction').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a690ac-959c-4b69-a170-1c5033490f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare test documents\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\", 1.0),\n",
    "    (5, \"l m n\", 0.0),\n",
    "    (6, \"spark hadoop spark\", 1.0),\n",
    "    (7, \"apache hadoop\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6efdb6b-d499-49bb-b64d-834a662444a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+----------------------+------------------------------------------------------+----------------------------------------+----------+\n|id |text              |label|words                 |features                                              |probability                             |prediction|\n+---+------------------+-----+----------------------+------------------------------------------------------+----------------------------------------+----------+\n|4  |spark i j k       |1.0  |[spark, i, j, k]      |(262144,[19036,68693,173558,213660],[1.0,1.0,1.0,1.0])|[0.6292098489668488,0.37079015103315116]|0.0       |\n|5  |l m n             |0.0  |[l, m, n]             |(262144,[1303,52644,248090],[1.0,1.0,1.0])            |[0.984770006762304,0.015229993237696027]|0.0       |\n|6  |spark hadoop spark|1.0  |[spark, hadoop, spark]|(262144,[173558,198017],[2.0,1.0])                    |[0.13412348342566147,0.8658765165743385]|1.0       |\n|7  |apache hadoop     |0.0  |[apache, hadoop]      |(262144,[68303,198017],[1.0,1.0])                     |[0.9955732114398529,0.00442678856014711]|0.0       |\n+---+------------------+-----+----------------------+------------------------------------------------------+----------------------------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "pred_test = model.transform(test)\n",
    "pred_test.drop('rawPrediction').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "402f7cc9-ed41-4fb6-bdcd-2e898170d214",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.75\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy on the test set\n",
    "predictionAndLabels = pred_test.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa299e1-dac9-46d5-b686-9e92973b5234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
