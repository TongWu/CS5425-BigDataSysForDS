{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f9f228-a6d4-4112-ac74-64a0b564ac85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# CS4225/CS5425Â Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dad939e-8336-4587-9473-fd7797652d23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Name: Wu Tong\n",
    "\n",
    "Matric Number: A0255954R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfc2f1b-1142-4a36-ae5a-3a1deca6548a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2276bdee-1da6-427c-a40d-f91cd3900aa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.1 Download Data (Citi Bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d466d1ca-b610-4425-a2ef-df835e841dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Function download CitiBike data\n",
    "def download_unzip(url, extract_to='/tmp/'):\n",
    "    file_name = url.split('/')[-1]\n",
    "    zip_path = os.path.join(extract_to, file_name)\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df1ed20-6895-497a-a93c-4b9b8d94d0e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call the function to download CitiBike data\n",
    "# 2022 whole-year data\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202201-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202202-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202203-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202204-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202205-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202206-citbike-tripdata.csv.zip')  # Typo!\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202207-citbike-tripdata.csv.zip')  # Typo!\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202208-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202209-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202210-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202211-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202212-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202301-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202302-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202303-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202304-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202305-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202306-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202307-citibike-tripdata.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e3096d-e164-4d0d-9512-c2719b0ff0a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2023 Jan-Jul data\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202301-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202302-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202303-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202304-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202305-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202306-citibike-tripdata.csv.zip')\n",
    "download_unzip('https://s3.amazonaws.com/tripdata/202307-citibike-tripdata.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04ef69a-880a-4190-91c9-e0747a68cf29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202201-citibike-tripdata.csv\n202202-citibike-tripdata.csv\n202203-citibike-tripdata.csv\n202204-citibike-tripdata.csv\n202205-citibike-tripdata.csv\n202206-citibike-tripdata.csv\n202207-citibike-tripdata.csv\n202208-citibike-tripdata.csv\n202209-citibike-tripdata.csv\n202210-citibike-tripdata.csv\n202211-citibike-tripdata.csv\n202212-citibike-tripdata.csv\n202301-citibike-tripdata.csv\n202302-citibike-tripdata.csv\n202303-citibike-tripdata.csv\n202304-citibike-tripdata.csv\n202305-citibike-tripdata.csv\n202306-citibike-tripdata.csv\n202307-citibike-tripdata.csv\nRserv\nRtmpoqZO75\n__MACOSX\nchauffeur-daemon-params\nchauffeur-daemon.pid\nchauffeur-env.sh\ncustom-spark.conf\ndriver-daemon-params\ndriver-daemon.pid\ndriver-env.sh\nhsperfdata_root\npython_lsp_logs\nsystemd-private-d3be94e18cbd44fdb03a6d333e00f28e-systemd-logind.service-qxGSti\nsystemd-private-d3be94e18cbd44fdb03a6d333e00f28e-systemd-resolved.service-fRLkvs\ntmp.oyJicEoAlm\n"
     ]
    }
   ],
   "source": [
    "%sh ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "145cca46-3668-443d-905d-9ce3f8fc75cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202201-citibike-tripdata.csv\n202202-citibike-tripdata.csv\n202203-citibike-tripdata.csv\n202204-citibike-tripdata.csv\n202205-citibike-tripdata.csv\n202206-citibike-tripdata.csv\n202207-citibike-tripdata.csv\n202208-citibike-tripdata.csv\n202209-citibike-tripdata.csv\n202210-citibike-tripdata.csv\n202211-citibike-tripdata.csv\n202212-citibike-tripdata.csv\n202301-citibike-tripdata.csv\n202302-citibike-tripdata.csv\n202303-citibike-tripdata.csv\n202304-citibike-tripdata.csv\n202305-citibike-tripdata.csv\n202306-citibike-tripdata.csv\n202307-citibike-tripdata.csv\nRserv\nRtmpoqZO75\n__MACOSX\nchauffeur-daemon-params\nchauffeur-daemon.pid\nchauffeur-env.sh\ncustom-spark.conf\ndriver-daemon-params\ndriver-daemon.pid\ndriver-env.sh\nhsperfdata_root\npython_lsp_logs\nsystemd-private-d3be94e18cbd44fdb03a6d333e00f28e-systemd-logind.service-qxGSti\nsystemd-private-d3be94e18cbd44fdb03a6d333e00f28e-systemd-resolved.service-fRLkvs\ntmp.oyJicEoAlm\n"
     ]
    }
   ],
   "source": [
    "# Rename the typo filename\n",
    "%%sh\n",
    "cd /tmp\n",
    "mv 202206-citbike-tripdata.csv 202206-citibike-tripdata.csv\n",
    "mv 202207-citbike-tripdata.csv 202207-citibike-tripdata.csv\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1536730-814b-40dc-a3fc-e7f48f6a260e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp\n"
     ]
    }
   ],
   "source": [
    "%sh ls /dbfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f76c2f60-ea00-42b2-bbc8-2d1d78e0e159",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1.1 Read CitiBike data into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d6fe6e-d37a-4db6-9532-7569741d1ed0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, min, to_date, datediff, to_timestamp, udf, count\n",
    "from pyspark.sql.types import DoubleType, IntegerType, FloatType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c320a6-00ff-4ea9-aaaa-ae370f0c4f03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.haversine(lon1, lat1, lon2, lat2)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    if None in (lon1, lat1, lon2, lat2):\n",
    "        return None\n",
    "    # Convert longtitude and latitude into radians\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # Haversine\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    r = 6371                        # Earth's average radius\n",
    "    return c * r\n",
    "\n",
    "spark.udf.register(\"haversine_udf\", haversine, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a730e0d-f854-4210-8be0-455074191be4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+------------------+\n|  datetime|trip_count|     trip_distance|         trip_time|\n+----------+----------+------------------+------------------+\n|2022-01-31|     22838| 36415.66340575871|381772.31666666653|\n|2022-01-29|      2927| 3679.015376442131| 90255.73333333332|\n|2022-01-15|     23717|32995.088187198315| 425115.9999999998|\n|2022-01-09|     22088|34282.984465227644| 795394.7333333325|\n|2022-01-18|     44045| 71586.66489926413|  632542.566666666|\n|2022-01-10|     35717|56102.318114016285|1117036.0666666655|\n|2022-01-08|     25129| 39177.86222436756|1151590.3500000008|\n|2022-01-24|     42591| 68813.54992986325| 506331.0333333338|\n|2022-01-23|     33817| 54528.93578195679| 512159.4666666667|\n|2022-01-16|     21588| 31728.70841590948|390820.96666666644|\n|2022-01-13|     51798| 89448.49425943883|  865903.033333333|\n|2022-01-06|     46004|  75891.3533867981|1334009.9000000004|\n|2022-01-03|     34126| 52264.02987379661| 1412850.700000001|\n|2022-01-25|     51982| 87618.33146901286| 644243.3499999994|\n|2022-01-30|      9688|13940.313887858289| 248816.4499999999|\n|2022-01-07|     17838|27226.572406360458| 742276.9833333334|\n|2022-01-22|     33018| 50196.96588989359| 529299.8999999993|\n|2022-01-04|     37879|  59053.2060579618|1840500.3000000003|\n|2022-01-17|     29887|46644.948374771906|414027.93333333294|\n|2022-01-28|     35740| 55539.86431966738| 456568.5833333332|\n+----------+----------+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Initialise a Spark session\n",
    "spark = SparkSession.builder.appName(\"CitiBikeDataAggregation\").getOrCreate()\n",
    "\n",
    "# Add 2022 whole-year files and 2023 Jan-Jul files\n",
    "months_2022 = [f\"{month:02d}\" for month in range(1, 13)]\n",
    "months_2023 = [f\"{month:02d}\" for month in range(1, 8)]\n",
    "file_list_2022 = [f'file:/tmp/2022{month}-citibike-tripdata.csv' for month in months_2022]\n",
    "file_list_2023 = [f'file:/tmp/2023{month}-citibike-tripdata.csv' for month in months_2023]\n",
    "file_list = file_list_2022 + file_list_2023\n",
    "combined_df = None\n",
    "\n",
    "for file_name in file_list:\n",
    "    # Read data\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "\n",
    "    # Convert started_at and ended_at col into timestamp col\n",
    "    df = df.withColumn('started_at', to_timestamp('started_at', 'yyyy/MM/dd HH:mm:ss'))\n",
    "    df = df.withColumn('ended_at', to_timestamp('ended_at', 'yyyy/MM/dd HH:mm:ss'))\n",
    "\n",
    "    # Calculate the trip distance and time\n",
    "    df = df.withColumn('trip_distance', haversine_udf(\n",
    "        col('start_lng'),\n",
    "        col('start_lat'),\n",
    "        col('end_lng'),\n",
    "        col('end_lat')\n",
    "    ))\n",
    "    df = df.withColumn('trip_time', (col('ended_at').cast(\"long\") - col('started_at').cast(\"long\")) / 60) # In\n",
    "\n",
    "    # Aggregate data for daily records\n",
    "    daily_df = df.groupBy(to_date(col('started_at')).alias('datetime')).agg(\n",
    "        count('*').alias('trip_count'),\n",
    "        sum('trip_distance').alias('trip_distance'),\n",
    "        sum('trip_time').alias('trip_time'),\n",
    "    )\n",
    "    combined_df = daily_df if combined_df is None else combined_df.union(daily_df)\n",
    "\n",
    "# Aggregate again\n",
    "final_df = combined_df.groupBy('datetime').agg(\n",
    "    sum('trip_count').alias('trip_count'),\n",
    "    sum('trip_distance').alias('trip_distance'),\n",
    "    sum('trip_time').alias('trip_time'),\n",
    ")\n",
    "\n",
    "final_df = final_df.withColumnRenamed(\"_c0\", \"datetime\") \\\n",
    "                   .withColumnRenamed(\"_c1\", \"trip_count\") \\\n",
    "                   .withColumnRenamed(\"_c2\", \"trip_distance\") \\\n",
    "                   .withColumnRenamed(\"_c3\", \"trip_time\")\n",
    "\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d26f00a-f746-460c-97de-aba0c504e115",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.2 Read Data (Weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7fdbed-b552-4f51-bc1a-07f1562ba579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherDataPreprocessing\").getOrCreate()\n",
    "\n",
    "# Read the weather dataset into a DataFrame\n",
    "weather_df = spark.read.csv(\"/FileStore/weather-1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select the required attributes\n",
    "selected_columns = [\n",
    "    \"datetime\", \"tempmax\", \"tempmin\", \"precip\", \"precipcover\", \n",
    "    \"snow\", \"snowdepth\", \"windspeed\", \"visibility\", \"uvindex\"\n",
    "]\n",
    "weather_df = weather_df.select(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3dbc4b-5f45-42c2-86f4-b5be2660add0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|  datetime|tempmax|tempmin|precip|precipcover|snow|snowdepth|windspeed|visibility|uvindex|\n+----------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|2022-01-01|   13.5|   10.0|18.463|      54.17| 0.0|      0.0|     13.1|       8.7|      1|\n|2022-01-02|   14.7|    3.6| 2.318|       12.5| 0.0|      0.0|     22.9|      11.8|      2|\n|2022-01-03|    3.3|   -4.7|   0.0|        0.0| 0.0|      0.0|     26.1|      16.0|      1|\n|2022-01-04|    1.3|   -6.9|   0.0|        0.0| 0.0|      0.0|     19.9|      16.0|      5|\n|2022-01-05|    8.1|   -0.5| 5.745|      33.33| 0.0|      0.0|     20.5|      13.5|      1|\n|2022-01-06|    5.2|    1.8|   0.0|        0.0| 7.4|      2.5|     21.3|      16.0|      4|\n|2022-01-07|    1.4|   -3.0| 9.314|      33.33| 4.4|     10.4|     24.0|      10.1|      5|\n|2022-01-08|   -1.2|   -6.2|   0.0|        0.0| 0.0|      8.2|     24.0|      16.0|      5|\n|2022-01-09|    4.3|   -1.5| 2.765|       25.0| 0.0|      5.4|     22.3|      14.8|      1|\n|2022-01-10|    4.7|   -3.6|   0.0|        0.0| 0.0|      1.7|     33.6|      15.9|      5|\n|2022-01-11|   -3.9|   -8.9|   0.0|        0.0| 0.0|      0.1|     35.3|      15.5|      5|\n|2022-01-12|    4.3|   -6.6|   0.0|        0.0| 0.0|      0.0|     22.3|      16.0|      5|\n|2022-01-13|    8.4|    1.8|   0.0|        0.0| 0.0|      0.0|     11.6|      15.7|      4|\n|2022-01-14|    5.7|   -4.2|   0.0|        0.0| 0.0|      0.0|     29.6|      15.9|      4|\n|2022-01-15|   -5.8|  -11.4|   0.0|        0.0| 0.0|      0.0|     22.4|      16.0|      5|\n|2022-01-16|    1.8|  -12.0| 5.546|      16.67| 0.4|      0.1|     29.3|      13.6|      4|\n|2022-01-17|    6.8|    1.4|26.429|      41.67| 0.0|      0.4|     34.9|      12.4|      1|\n|2022-01-18|    2.5|   -1.0|   0.0|        0.0| 0.0|      0.0|     28.8|      16.0|      5|\n|2022-01-19|    9.5|   -1.5|   0.0|        0.0| 0.0|      0.0|     22.3|      16.0|      4|\n|2022-01-20|    7.5|   -4.2| 6.116|       37.5| 0.9|      0.3|     22.3|      12.6|      2|\n+----------+-------+-------+------+-----------+----+---------+---------+----------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88155339-de0d-4371-9a8f-40761d90d581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|datetime|tempmax|tempmin|precip|precipcover|snow|snowdepth|windspeed|visibility|uvindex|\n+--------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|       0|      0|      0|     0|          0|   0|        0|        0|         0|      0|\n+--------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing column\n",
    "from pyspark.sql.functions import col, count, when, isnull\n",
    "\n",
    "missing_data_count = weather_df.select([count(when(isnull(c), c)).alias(c) for c in weather_df.columns])\n",
    "missing_data_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff94ee14-fb84-4384-8c9c-aaf5aacc3cca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|  datetime|tempmax|tempmin|precip|precipcover|snow|snowdepth|windspeed|visibility|uvindex|\n+----------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|2022-01-01|   13.5|   10.0|18.463|      54.17| 0.0|      0.0|     13.1|       8.7|      1|\n|2022-01-02|   14.7|    3.6| 2.318|       12.5| 0.0|      0.0|     22.9|      11.8|      2|\n|2022-01-03|    3.3|   -4.7|   0.0|        0.0| 0.0|      0.0|     26.1|      16.0|      1|\n|2022-01-04|    1.3|   -6.9|   0.0|        0.0| 0.0|      0.0|     19.9|      16.0|      5|\n|2022-01-05|    8.1|   -0.5| 5.745|      33.33| 0.0|      0.0|     20.5|      13.5|      1|\n|2022-01-06|    5.2|    1.8|   0.0|        0.0| 7.4|      2.5|     21.3|      16.0|      4|\n|2022-01-07|    1.4|   -3.0| 9.314|      33.33| 4.4|     10.4|     24.0|      10.1|      5|\n|2022-01-08|   -1.2|   -6.2|   0.0|        0.0| 0.0|      8.2|     24.0|      16.0|      5|\n|2022-01-09|    4.3|   -1.5| 2.765|       25.0| 0.0|      5.4|     22.3|      14.8|      1|\n|2022-01-10|    4.7|   -3.6|   0.0|        0.0| 0.0|      1.7|     33.6|      15.9|      5|\n|2022-01-11|   -3.9|   -8.9|   0.0|        0.0| 0.0|      0.1|     35.3|      15.5|      5|\n|2022-01-12|    4.3|   -6.6|   0.0|        0.0| 0.0|      0.0|     22.3|      16.0|      5|\n|2022-01-13|    8.4|    1.8|   0.0|        0.0| 0.0|      0.0|     11.6|      15.7|      4|\n|2022-01-14|    5.7|   -4.2|   0.0|        0.0| 0.0|      0.0|     29.6|      15.9|      4|\n|2022-01-15|   -5.8|  -11.4|   0.0|        0.0| 0.0|      0.0|     22.4|      16.0|      5|\n|2022-01-16|    1.8|  -12.0| 5.546|      16.67| 0.4|      0.1|     29.3|      13.6|      4|\n|2022-01-17|    6.8|    1.4|26.429|      41.67| 0.0|      0.4|     34.9|      12.4|      1|\n|2022-01-18|    2.5|   -1.0|   0.0|        0.0| 0.0|      0.0|     28.8|      16.0|      5|\n|2022-01-19|    9.5|   -1.5|   0.0|        0.0| 0.0|      0.0|     22.3|      16.0|      4|\n|2022-01-20|    7.5|   -4.2| 6.116|       37.5| 0.9|      0.3|     22.3|      12.6|      2|\n+----------+-------+-------+------+-----------+----+---------+---------+----------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Handle the missing data\n",
    "from pyspark.sql.functions import mean, col\n",
    "\n",
    "# Contineous data\n",
    "numerical_columns = [\"tempmax\", \"tempmin\", \"precip\", \"precipcover\", \"snow\", \"snowdepth\", \"windspeed\", \"visibility\", \"uvindex\"]\n",
    "\n",
    "# Calculate mean to fill the missing value\n",
    "for column in numerical_columns:\n",
    "    mean_value = weather_df.select(mean(col(column))).collect()[0][0]\n",
    "    weather_df = weather_df.na.fill({column: mean_value})\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c29f44-2194-43bb-a38a-34624bd56d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.3 Extra Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d0c52e-6e12-47b5-a585-7e20df7abf22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "No extra data is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62022f5e-dc70-4211-87b8-92106bdfc7f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.4 Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24451737-352a-4ef7-b639-ad683dc7e7fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Join the DataFrames on the `datetime` column\n",
    "combined_df = final_df.join(weather_df, \"datetime\")\n",
    "\n",
    "# Save the combined DataFrame to DBFS\n",
    "combined_df.write.format(\"parquet\").save(\"/tmp/combined_data.parquet\")\n",
    "\n",
    "# Read the table from DBFS to ensure it's ready for modelling\n",
    "combined_df = spark.read.format(\"parquet\").load(\"/tmp/combined_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886ce8de-046a-49a9-a344-0b14c0354c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+------------------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|  datetime|trip_count|     trip_distance|         trip_time|tempmax|tempmin|precip|precipcover|snow|snowdepth|windspeed|visibility|uvindex|\n+----------+----------+------------------+------------------+-------+-------+------+-----------+----+---------+---------+----------+-------+\n|2022-01-31|     22838| 36415.66340575871|381772.31666666653|   -0.9|   -6.1|   0.0|        0.0| 0.0|      8.2|     12.1|      16.0|      6|\n|2022-01-29|      2927| 3679.015376442131| 90255.73333333332|   -1.5|  -10.2|15.831|      66.67| 7.0|     12.9|     29.7|       6.0|      1|\n|2022-01-15|     23717|32995.088187198315| 425115.9999999998|   -5.8|  -11.4|   0.0|        0.0| 0.0|      0.0|     22.4|      16.0|      5|\n|2022-01-09|     22088|34282.984465227644| 795394.7333333325|    4.3|   -1.5| 2.765|       25.0| 0.0|      5.4|     22.3|      14.8|      1|\n|2022-01-18|     44045| 71586.66489926413|  632542.566666666|    2.5|   -1.0|   0.0|        0.0| 0.0|      0.0|     28.8|      16.0|      5|\n|2022-01-10|     35717|56102.318114016285|1117036.0666666655|    4.7|   -3.6|   0.0|        0.0| 0.0|      1.7|     33.6|      15.9|      5|\n|2022-01-08|     25129| 39177.86222436756|1151590.3500000008|   -1.2|   -6.2|   0.0|        0.0| 0.0|      8.2|     24.0|      16.0|      5|\n|2022-01-24|     42591| 68813.54992986325| 506331.0333333338|    1.8|   -2.6|   0.0|        0.0| 0.0|      0.1|     16.3|      14.7|      4|\n|2022-01-23|     33817| 54528.93578195679| 512159.4666666667|    2.5|   -4.1|  0.52|       12.5| 0.0|      0.0|     20.5|      14.6|      5|\n|2022-01-16|     21588| 31728.70841590948|390820.96666666644|    1.8|  -12.0| 5.546|      16.67| 0.4|      0.1|     29.3|      13.6|      4|\n|2022-01-13|     51798| 89448.49425943883|  865903.033333333|    8.4|    1.8|   0.0|        0.0| 0.0|      0.0|     11.6|      15.7|      4|\n|2022-01-06|     46004|  75891.3533867981|1334009.9000000004|    5.2|    1.8|   0.0|        0.0| 7.4|      2.5|     21.3|      16.0|      4|\n|2022-01-03|     34126| 52264.02987379661| 1412850.700000001|    3.3|   -4.7|   0.0|        0.0| 0.0|      0.0|     26.1|      16.0|      1|\n|2022-01-25|     51982| 87618.33146901286| 644243.3499999994|    5.8|   -0.8|   0.0|        0.0| 0.0|      0.0|     19.8|      14.7|      2|\n|2022-01-30|      9688|13940.313887858289| 248816.4499999999|   -3.3|  -10.7|   0.0|        0.0| 1.3|     16.1|     29.7|      16.0|      6|\n|2022-01-07|     17838|27226.572406360458| 742276.9833333334|    1.4|   -3.0| 9.314|      33.33| 4.4|     10.4|     24.0|      10.1|      5|\n|2022-01-22|     33018| 50196.96588989359| 529299.8999999993|   -1.5|  -10.0|   0.0|        0.0| 0.0|      0.0|     18.1|      16.0|      5|\n|2022-01-04|     37879|  59053.2060579618|1840500.3000000003|    1.3|   -6.9|   0.0|        0.0| 0.0|      0.0|     19.9|      16.0|      5|\n|2022-01-17|     29887|46644.948374771906|414027.93333333294|    6.8|    1.4|26.429|      41.67| 0.0|      0.4|     34.9|      12.4|      1|\n|2022-01-28|     35740| 55539.86431966738| 456568.5833333332|    0.2|   -1.5| 1.702|      29.17| 8.1|      2.7|     21.0|       8.1|      1|\n+----------+----------+------------------+------------------+-------+-------+------+-----------+----+---------+---------+----------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2685471e-346a-4982-ab91-25878b8d5c30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.5 Filter the final table / datafram into training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8ef7ab-767a-44a7-8273-dd076d0ef6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 365\nTesting Data Count: 212\n"
     ]
    }
   ],
   "source": [
    "# Now, we filter the combined DataFrame into training and testing datasets\n",
    "training_data = combined_df.filter(year(\"datetime\") == 2022)\n",
    "testing_data = combined_df.filter(year(\"datetime\") == 2023)\n",
    "\n",
    "# Assuming you want to save these to DBFS as well\n",
    "training_data.write.format(\"parquet\").save(\"/tmp/training_data.parquet\")\n",
    "testing_data.write.format(\"parquet\").save(\"/tmp/testing_data.parquet\")\n",
    "\n",
    "# Verify the data count\n",
    "print(f\"Training Data Count: {training_data.count()}\")\n",
    "print(f\"Testing Data Count: {testing_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122f1731-f2ee-4352-ae6a-c581fc7e62a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Build ML Pipeline using Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee94ad40-0b4c-47a0-99e4-93d21a9803d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50bf8d59-dbe2-413b-a5a5-d29439cad370",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MLPipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00beafda-8118-4839-a21c-78d0d0317ae2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1  Encode the categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5778ed76-985b-4cc7-bde3-b310301fac93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There is no features can be encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37f3841-87cd-40eb-a55d-e2a45fb72ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2 Scaling down the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69485ce7-6b59-4cd4-bcb6-27bb178a7053",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature vector from the input features\n",
    "feature_columns = [\n",
    "    \"tempmax\", \"tempmin\", \"precip\", \"precipcover\", \"snow\",\n",
    "    \"snowdepth\", \"windspeed\", \"visibility\", \"uvindex\", \n",
    "    \"trip_distance\", \"trip_time\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb98a092-316b-4073-b903-f10d50a7ad74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.3 Select the revelant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af57f9e4-7082-4b06-8eea-4cde5856ed28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The selection of the revelant features is partly done in data preprocessing stage, and more on the scaling and assembing stage above and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710416d0-e4d4-499f-b967-d198dd98d6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assembler select the revelant features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc0aa672-161d-4234-9352-e715f394e0be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.4 Build ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379dd35b-fbe1-40cd-b677-c55bddd5da1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Random Forest to build the model\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"trip_count\")\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339bc069-d8f8-4c5e-8095-e63680985cd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Evaluate ML Pipeline using Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1207950-6146-4281-83ff-e4d9ab970611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1610c3ca-a4aa-4175-bd95-8c397b35d318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+\n|  datetime|predicted_trip_count|actual_trip_count|\n+----------+--------------------+-----------------+\n|2023-01-01|   60288.51366861246|            52561|\n|2023-01-02|  63914.894886709735|            60369|\n|2023-01-28|   69809.56798149788|            68844|\n|2023-01-11|   59255.77927570346|            69327|\n|2023-01-12|   52099.86510083667|            53216|\n|2023-01-25|    39050.8324158758|            43222|\n|2023-01-21|  58016.093561969115|            60243|\n|2023-01-26|    60532.5170215664|            66678|\n|2023-01-05|    71201.7191154671|            74016|\n|2023-01-31|   53370.89711694243|            59258|\n|2023-01-03|   52646.97099661912|            53591|\n|2023-01-13|   63121.32245717246|            67539|\n|2023-01-14|  46559.359639975235|            44968|\n|2023-01-06|  62877.039757549566|            67023|\n|2023-01-22|   41989.99571537206|            40698|\n|2023-01-04|   75680.51359480638|            76968|\n|2023-01-07|  63127.273123125546|            61270|\n|2023-01-24|   66355.58426943017|            69870|\n|2023-01-20|   66828.96876931484|            69848|\n|2023-01-27|   63071.62674559307|            67796|\n+----------+--------------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testing_data)\n",
    "results_df = predictions.select(\n",
    "    col(\"datetime\"),\n",
    "    col(\"prediction\").alias(\"predicted_trip_count\"),\n",
    "    col(\"trip_count\").alias(\"actual_trip_count\")\n",
    ")\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d214ec5-5271-4dfd-8de7-d5f115ae10c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5421.270145175778\nRMSE: 6534.847879410365\nR-squared: 0.9536338661563291\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"trip_count\", predictionCol=\"prediction\")\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d1d77a-b2ac-42ed-a153-027637c43752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"/tmp/citibike_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c24bc7b-1a1f-47b9-83ea-36852f7535c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 998587398719987,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "assignment_2_student",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
